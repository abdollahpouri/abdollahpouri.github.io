<!DOCTYPE HTML>
<!--
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Maria Dimakopoulou - Homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="landing">

		<!-- Header -->
			<header id="header" class="alt">
				<h1><strong><a href="index.html">Stanford </a></strong>Reinforcement Learning</h1>
				<nav id="nav">
					<ul>
						<li><a href="index.html">Home</a></li>
						<!-- <li><a href="research.html">Research</a></li> -->
						<li><a href="assets/docs/CV.pdf">CV</a></li>
					</ul>
				</nav>
			</header>

			<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

		<!-- Banner -->
			<section id="banner">
				<h2>Maria Dimakopoulou</h2>
				<!-- <p>Reinforcement Learning<br /> Mice chaser</p> -->
			</section>

			<!-- About me -->
				<section id="one" class="wrapper style1">
					<div class="container 75%">
						<div class="row 200%">
							<div class="6u 12u$(medium)">
								<header class="major">
									<h2>About Me</h2>
									<div class="11u"><span class="image fit"><img src="images/maria.jpg" style="max-width: 750px" alt="" /></span></div>
								</header>
							</div>
							<div class="6u$ 12u$(medium)">
								<p align="justify">
								I am currently a Senior Research Scientist at <a href="https://research.netflix.com/" target="_blank">Netflix and my research focuses on reinforcement learning, contextual bandits and causal inference for personalization.
								Previously, I obtained my PhD in <a href="https://msande.stanford.edu/" target="_blank">Management Science &amp; Engineering department</a> at <a href="https://stanford.edu/" target="_blank"> Stanford University</a>, 
								where I was advised by  <a href="http://web.stanford.edu/~bvr/" target="_blank">Benjamin Van Roy</a> and <a href="https://people.stanford.edu/athey/" target="_blank">Susan Athey</a>. 
								
								<p align="justify">
								In 2015, I received a MSc in Operations Research at Stanford University, graduating first of my class. 
								In 2014, I received a BSc and MSc in Electrical Engineering &amp; Computer Science from National Technical University of Athens (<a href="http://www.ntua.gr/en/" target="_blank">NTUA</a>), Greece, graduating with the highest GPA in NTUA's 200-year history. 
								Between 2012 and 2015, I spent time at <a href="http://research.google.com" target="_blank">Google Research</a>, where I worked on the design and deployment of large-scale optimization algorithms for Google Technical Infrastructure and Google Ad Exchange. 
								In 2016, I led the design and launched the multi-touch attribution product of <a href="http://www.krux.com" target="_blank">Krux</a> (now Salesforce Einstein).
								In 2018, I joined the <a href="https://www.microsoft.com/en-us/research/group/machine-learning-nyc/" target="_blank">Machine Learning group at Microsoft Research NYC</a>, where I worked with Miroslav Dudik and Robert Schapire on reinforcement learning decomposition and off-policy evaluation for high-dimensional contextual bandits.
								
								<p align="justify">
								I have been the recipient of the &quot;<a href="https://www.linkedin.com/in/nick-arvanitidis-332b541" target="_blank">Arvanitidis</a>&quot; <a href="https://vpge.stanford.edu/fellowships-funding/sgf" target="_blank"> Stanford Graduate Fellowship</a> in Memory of <a href="https://en.wikipedia.org/wiki/William_Linvill" target="_blank">William K. Linvill</a>,
								the <a href="http://www.onassis.org/en/" target="_blank">Onassis Foundation Graduate Fellowship</a>.
								the Intel Honorary Award, the Google <a href="http://anitab.org/awards-grants/" target="_blank">Anita Borg Memorial Award</a>, the Google 
								Excellence Award, the Stanford Graduate Fellowship in Science and Engineering, and the Stanford Outstanding Academic Achievement Award.</p>
								
								<p align="justify">
								I enjoy swimming, travelling across the world with great company, and exploring impressionist and surrealist art.</p>

							</div>
						</div>
					</div>
				</section>

			<!-- Two -->
<!--
			<section id="news" class="wrapper style2 special">
                			<div class="container" style="max-width: 1000px; line-height: 140%;">

                    			<header class="major special">
                        			<h2>News</h2>
                    			</header>

				        <p id=""><span class="image left"><img src="images/empire.jpg" title="" alt="" style="max-width: 140px"></span>
<p>From June to September 2018, I will join the Machine Learning group at Microsoft Research NYC.</p>
					</div>
			</section>
-->
			<!-- Three -->
				<section id="three" class="wrapper style2 special">
					<div class="container">
						<header class="major special">
							<h2>Research</h2>
							<!-- <p>Machine Learning and more...</p> -->
						</header>
						<div class="feature-grid">
						
						
											<div class="feature">
								<div class="image rounded">
								  <img src="images/balanced.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Balanced Linear Contextual Bandits</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Zhengyuan&nbsp;Zhou, Susan&nbsp;Athey, Guido&nbsp;Imbens (AAAI&nbsp;2019;&nbsp;oral)</i></p>
Contextual bandit algorithms are sensitive to the estimation method of the outcome model as well as the exploration method used, particularly 
in the presence of rich heterogeneity or complex outcome models, which can lead to difficult estimation problems along the path of learning. 
We develop algorithms for contextual bandits with linear payoffs that integrate balancing methods from the causal inference literature in their 
estimation to make it less prone to problems of estimation bias. We provide the first regret bound analyses for linear contextual bandits with 
balancing and show that our algorithms match the state of the art theoretical guarantees. We demonstrate the strong practical advantage of balanced 
contextual bandits on a large number of supervised learning datasets and on a synthetic example that simulates model misspecification and prejudice in the initial training data.<br>
[<a href="https://arxiv.org/pdf/1812.06227.pdf" target="_blank">Paper</a>][<a href="assets/docs/AAAI2019_poster.pdf">Poster</a>]
</p>
								</div>
							</div>

													<div class="feature">
								<div class="image rounded">
								  <img src="images/robots.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Scalable Coordinated Exploration in Concurrent Reinforcement Learning</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Ian&nbsp;Osband, Benjamin&nbsp;Van&nbsp;Roy (NIPS&nbsp;2018)</i></p>
We consider a team of reinforcement learning agents that concurrently operate in a
common environment, and we develop an approach to efficient coordinated exploration
that is suitable for problems of practical scale. Our approach builds on seed sampling (Dimakopoulou and Van Roy, 2018)
and randomized value function learning (Osband et al., 2016). We demonstrate that, for simple tabular
contexts, the approach is competitive with previously proposed tabular model learning
methods (Dimakopoulou and Van Roy, 2018). With a higher-dimensional problem and a neural network value function
representation, the approach learns quickly with far fewer agents than alternative
exploration schemes.<br>
[<a href="https://arxiv.org/pdf/1805.08948.pdf" target="_blank">Paper</a>][<a href="assets/docs/NIPS2018_poster.pdf">Poster</a>]
</p>
								</div>
							</div>

							<div class="feature">
								<div class="image rounded">
								  <img src="images/cubist.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Coordinated Exploration in Concurrent Reinforcement Learning</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Benjamin&nbsp;Van&nbsp;Roy (ICML&nbsp;2018;&nbsp;long&nbsp;talk)</i></p>
<p>We consider a team of reinforcement learning agents that concurrently learn to operate in a common environment, while sharing data in real-time. We identify three properties that are essential to efficient coordinated exploration: real-time adaptivity to shared observations, commitment to carry through with action sequences that reveal new information, and diversity across learning opportunities pursued by different agents. We demonstrate that optimism-based approaches fall short with respect to diversity, while naive extensions of Thompson sampling lack commitment. We propose seed sampling that offers a general approach to designing effective coordination algorithms for concurrent reinforcement learning and has substantial advantages over alternative exploration schemes.<br>
[<a href="http://proceedings.mlr.press/v80/dimakopoulou18a/dimakopoulou18a.pdf" target="_blank">Paper</a>]
[<a href="https://youtu.be/xjGK-wm0PkI" target="_blank">Demo</a>]
[<a href="https://docs.google.com/presentation/d/1tKztxH5FaH2IRVtCu1escC3O51u26ok05A3xlSSwWAY/edit#slide=id.g3d32e7e0e3_0_129" target="_blank">ICML 2018 Slides</a>] [<a href="https://www.facebook.com/icml.imls/videos/429761510871703/">ICML 2018 Video</a>]
</p>
								</div>
							</div>
							<div class="feature">
								<div class="image rounded">
								  <img src="images/thinker.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Estimation Considerations in Contextual Bandits</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Zhengyuan&nbsp;Zhou, Susan&nbsp;Athey, Guido&nbsp;Imbens</i></p>
<p> We study a new consideration for the exploration vs. exploitation framework which is that the way exploration is conducted in the present may affect the bias and variance in the potential outcome model estimation in subsequent stages of learning. We show that contextual bandit algorithms are sensitive to the estimation method of the outcome model as well as the exploration method used, particularly in the presence of rich heterogeneity or complex outcome models, which can lead to difficult estimation problems along the path of learning. We propose new contextual bandit designs, combining parametric and nonparametric statistical estimation methods with causal inference methods in order to reduce the estimation bias and provide empirical evidence that guides the choice among the alternatives in different scenarios.<br>
[<a href="https://arxiv.org/abs/1711.07077" target="_blank">Paper</a>]
</p>
								</div>
							</div>
							
														<div class="feature">
								<div class="image rounded">
								  <img src="images/wireless.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Market-based dynamic service mode switching in wireless networks</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Nicholas&nbsp;Bambos, Martin&nbsp;Valdez-Vivas, John&nbsp;Apostolopoulos (PIMRC&nbsp;2017)</i></p>
<p>We consider a virtualized wireless networking architecture, where infrastructure access points of different carriers form a marketplace of resources and bid service deals to a mobile device. At each point in time the mobile evaluates the available service deals and dynamically decides which one to accept and use in the next transmission interval. Its objective is to minimize the long term cumulative service cost and latency cost to transmit packets in its buffer. We develop a model of this architecture, which allows for the formulation and computation of the optimal control for the mobile to accept an offered deal amongst many and switch into the corresponding service mode. The performance of the optimal
and low-complexity heuristic controls is probed via simulation.<br>
[<a href="http://ieeexplore.ieee.org/document/8292195/" target="_blank">Paper</a>]
</p>
								</div>
							</div>
							<div class="feature">
								<div class="image rounded">
								  <img src="images/dc.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Reliable and Efficient Performance Monitoring in Linux</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, St&eacute;phane&nbsp;Eranian, Nectarios&nbsp;Koziris, Nicholas&nbsp;Bambos (Supercomputing&nbsp;2016)</i></p>
<p>We address a published eratum in the Performance Monitoring Unit (PMU) of Intel Sandy Bridge, Ivy Bridge and Haswell processors with hyper-threading enabled which causes cross hyper-thread hardware counter corruption and may produce unreliable results. We propose a cache-coherence style protocol, which we implement in the Linux kernel to address the issue by introducing cross hyper-thread dynamic event scheduling. Additionally, we improve event scheduling efficiency by introducing a bipartite graph matching algorithm which optimally schedules events onto hardware counters consistently. The improvements have been contributed to the upstream Linux kernel&nbsp;v4.1.<br>
[<a href="http://ieeexplore.ieee.org/document/7877112/" target="_blank">Paper</a>]
</p>
								</div>
							</div>


						</div>
					</div>
				</section>
 <!-- teaching -->
            <section id="teaching" class="wrapper special">
                <div class="container" style="max-width: 1000px; line-height: 140%;">

                    <header class="major special">
                        <h2>News</h2>
                    </header>

<h4>10/9/2018</h4>
<p>I defended my PhD thesis! <br> The committee of my defense was Benjamin Van Roy, Susan Athey, Emma Brunskill, Balaji Prabhakar and Guido Imbens. [<a href="images/defense.jpeg" target="_blank">Photo</a>]</p>

					
<h4>9/4/2018</h4>
<p>The paper "Scalable Coordinated Exploration in Concurrent Reinforcement Learning" has been accepted to NIPS 2018.</p>
					
<h4>8/26/2018</h4>
<p>A <a href="https://www.youtube.com/watch?v=kwvhfzbzb0o" target="_blank"> new demo</a> has been uploaded showcasing seed sampling with generalization from the paper <br> "Scalable Coordinated Exploration in Concurrent Reinforcement Learning".</p>
					
<h4>7/11/2018</h4>
<p>The long talk I gave in ICML 2018 on "Coordinated Exploration in Concurrent Reinforcement Learning". [<a href="https://docs.google.com/presentation/d/1tKztxH5FaH2IRVtCu1escC3O51u26ok05A3xlSSwWAY/edit#slide=id.g3d32e7e0e3_0_129" target="_blank">Slides</a>] [<a href="https://www.facebook.com/icml.imls/videos/429761510871703/">Video</a>]</p>
					
<h4>5/25/2018</h4>
<p>The slides from my seminar talk at Netflix can be found <a href="https://docs.google.com/presentation/d/1c9AFvRdpaLNvsUscZFVGCYUBnjbFrSM_QFEOCvttBqU/" target="_blank">here</a>.</p>
					
<h4>3/5/2018</h4>
<p>An <a href="https://youtu.be/xjGK-wm0PkI" target="_blank"> animated demo</a> has been uploaded showcasing seed sampling from the paper <br> "Coordinated Exploration in Concurrent Reinforcement Learning".</p>

<h4>2/2/2018</h4>
<p>From June to September 2018, I will join the <a href="https://www.microsoft.com/en-us/research/group/machine-learning-nyc/" target="_blank">Machine Learning group at Microsoft Research NYC</a>.</p>


            </section>


			<!-- Four -->
				<section id="four" class="wrapper style3 special">
					<div class="container">
						<header class="major">
							<h2>Get in touch</h2>
						</header>
					</div>
				</section>

		<!-- Footer -->
			<footer id="footer">
				<div class="container">
					<ul class="icons">
						<li><a href="mailto:madima@stanford.edu" class="icon fa-mail-reply"></a></li>
						<!-- <li><a href="https://github.com/mdimakopoulou" class="icon fa-github"></a></li> -->
						<li><a href="https://www.linkedin.com/in/maria-dimakopoulou-4567428a/" class="icon fa-linkedin"></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Maria Dimakopoulou</li>
						<li>Design: <a href="http://templated.co">TEMPLATED</a></li>
						<!-- <li>Images: <a href="http://unsplash.com">Unsplash</a></li> -->
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
