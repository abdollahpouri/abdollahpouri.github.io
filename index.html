<!DOCTYPE HTML>
<!--
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Maria Dimakopoulou - Homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="landing">

		<!-- Header -->
			<header id="header" class="alt">
				<p><h1><strong><a href="index.html">Stanford </a></strong>Reinforcement Learning</h1></p>
				<nav id="nav">
					<ul>
						<li><a href="index.html">Home</a></li>
						<!-- <li><a href="research.html">Research</a></li> -->
						<li><a href="assets/docs/CV.pdf">CV</a></li>
					</ul>
				</nav>
			</header>

			<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

		<!-- Banner -->
			<section id="banner">
				<h2>Maria Dimakopoulou</h2>
				<!-- <p>Reinforcement Learning<br /> Mice chaser</p> -->
			</section>

			<!-- About me -->
				<section id="one" class="wrapper style1">
					<div class="container 75%">
						<div class="row 200%">
							<div class="6u 12u$(medium)">
								<header class="major">
									<h2>About Me</h2>
									<div class="11u"><span class="image fit"><img src="images/maria.jpg" style="max-width: 750px" alt="" /></span></div>
								</header>
							</div>
							<div class="6u$ 12u$(medium)">
								<p align="justify">
								I am a Senior Research Scientist at <a href="https://research.netflix.com/" target="_blank">Netflix</a> and my research focuses on bandit learning, reinforcement learning and causal inference. I lead the Adaptive Experimentation research agenda, developing research methodologies that become part of production and improve the personalized ranking algorithms and the A/B testing at Netflix.
								Previously, I obtained my PhD in reinforcement learning at <a href="https://stanford.edu/" target="_blank"> Stanford University</a>, 
								where I was advised by  <a href="http://web.stanford.edu/~bvr/" target="_blank">Benjamin Van Roy</a> and <a href="https://people.stanford.edu/athey/" target="_blank">Susan Athey</a>. 
								
								<p align="justify">
								In 2015, I received a MSc in Operations Research at Stanford University, graduating first of my class. 
								In 2014, I received a BSc and MSc in Electrical Engineering &amp; Computer Science from National Technical University of Athens (<a href="https://en.wikipedia.org/wiki/National_Technical_University_of_Athens" target="_blank">NTUA</a>), Greece, graduating with the highest GPA in NTUA's 200-year history. 
								Between 2012 and 2015, I worked at <a href="http://research.google.com" target="_blank">Google Research</a>, focusing on the design and deployment of large-scale optimization algorithms for Google Technical Infrastructure and Google Ad Exchange. 
								In 2016, I was a visiting researcher at <a href="http://www.krux.com" target="_blank">Krux</a> (now Salesforce Einstein), where I led the design of and launched the company's multi-touch attribution product. 
								In 2018, I was a visiting researcher at <a href="https://www.microsoft.com/en-us/research/group/machine-learning-nyc/" target="_blank">Microsoft Research NYC</a>, where I worked with Miroslav Dudik and Robert Schapire on reinforcement learning and off-policy evaluation.
								
								<p align="justify">
								I have been the recipient of the Stanford Outstanding Academic Achievement Award in 2019 and 2016, 
								the Forbes 30 Under 30 Greece in 2019, 
								the "Arvanitidis" Stanford Doctoral Fellowship from 2015 to 2018,
								the Onassis Foundation Doctoral Fellowship from 2015 to 2018,
								the Intel Honorary Award in 2014,
								the Google Women Techmakers Award in 2014
								and the Google Excellence Award in 2013. 								
								
								<p align="justify">
								I enjoy swimming, travelling across the world with great company, and exploring impressionist and surrealist art.</p>

							</div>
						</div>
					</div>
				</section>

			<!-- Two -->
<!--
			<section id="news" class="wrapper style2 special">
                			<div class="container" style="max-width: 1000px; line-height: 140%;">

                    			<header class="major special">
                        			<h2>News</h2>
                    			</header>

				        <p id=""><span class="image left"><img src="images/empire.jpg" title="" alt="" style="max-width: 140px"></span>
<p>From June to September 2018, I will join the Machine Learning group at Microsoft Research NYC.</p>
					</div>
			</section>
-->
			<!-- Three -->
				<section id="three" class="wrapper style2 special">
					<div class="container">
						<header class="major special">
							<h2>Research</h2>
							<!-- <p>Machine Learning and more...</p> -->
						</header>
						<div class="feature-grid">
		
<div class="feature">
								<div class="image rounded">
								  <img src="images/design.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Doubly Robust Off-Policy Evaluation with Shrinkage</h4>
									</header>
<p><i>Yi&nbsp;Su, Maria&nbsp;Dimakopoulou, <br>Akshay&nbsp;Krishnamurthy, Miroslav&nbsp;Dudik (ICML 2020)</i></p>
We design a new family of estimators for off-policy evaluation in contextual bandits. 
Our estimators are based on the asymptotically optimal approach of doubly robust estimation, 
but they shrink importance weights to obtain a better bias-variance tradeoff in finite samples. 
Our approach adapts importance weights to the quality of a reward predictor, interpolating 
between doubly robust estimation and direct modeling. When the reward predictor is poor, 
we recover previously studied weight clipping, but when the reward predictor is good, we 
obtain a new form of shrinkage. Extensive experiments on bandit benchmark problems show that our 
estimators are highly adaptive and typically outperform state-of-the-art methods.<br>
[<a href="https://proceedings.icml.cc/static/paper_files/icml/2020/3992-Paper.pdf" target="_blank">Paper</a>]
</p>
								</div>
							</div>
					

<div class="feature">
								<div class="image rounded">
								  <img src="images/recs.png" alt="" /></div>
								<div class="content">
									<header>
										<h4>ADMM SLIM: Sparse Recommendations for Many Users</h4>
									</header>
<p><i>Harald&nbsp;Steck, Maria&nbsp;Dimakopoulou, <br>Nickolai&nbsp;Riabov, Tony&nbsp;Jebara (WSDM 2020)</i></p>
The Sparse Linear Method (SLIM) is a well-established approach for top-N recommendations. 
This article proposes several improvements to SLIM that are enabled by the Alternating Directions
Method of Multipliers (ADMM). 
We evaluate our approach against the original SLIM and other state-of-the-art approaches on 
three well-known data-sets.
In our experiments, we find that not only our approach reduces training time considerably 
but also achieves an up to 25% improvement in recommendation accuracy  due to better optimization. 
Finally, we compare the approaches in experiments that simulate scenarios of cold-starting and large catalog sizes compared 
to relatively small user base, which often occur in practice.<br>
[<a href="assets/docs/WSDM2020.pdf">Paper</a>]
</p>
								</div>
							</div>
<div class="feature">
								<div class="image rounded">
								  <img src="images/slate.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Marginal Posterior Sampling for Slate Bandits</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Nikos&nbsp;Vlassis, Tony&nbsp;Jebara (IJCAI&nbsp;2019)</i></p>
We introduce a new Thompson sampling-based algorithm, called marginal posterior sampling, for online slate bandits, that is characterized by three key ideas. 
First, it postulates that the slate-level reward is a monotone function of the marginal unobserved rewards of the actions in the slate's slots, which it does not attempt to estimate. 
Second, it maintains posterior distributions for the marginal reward of each slot's actions rather than a slate-level reward posterior. 
Third, it optimizes at the slot-level rather than the slate-level, which makes it computationally efficient. 
Simulation results show substantial advantages of marginal posterior sampling over state-of-the-art alternatives.<br>
[<a href="https://www.ijcai.org/proceedings/2019/0308.pdf" target="_blank">Paper</a>]
</p>
								</div>
							</div>


<div class="feature">
								<div class="image rounded">
								  <img src="images/balanced.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Balanced Linear Contextual Bandits</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Zhengyuan&nbsp;Zhou, Susan&nbsp;Athey, Guido&nbsp;Imbens (AAAI&nbsp;2019;&nbsp;oral)</i></p>
Contextual bandit algorithms are sensitive to the estimation method of the outcome model as well as the exploration method used, particularly 
in the presence of rich heterogeneity or complex outcome models. 
We develop algorithms for contextual bandits with linear payoffs that integrate balancing methods from the causal inference literature in their 
estimation to make it less prone to problems of  bias. We prove that our algorithms match the state of the art regret bound guarantees. We demonstrate the strong practical advantage of balanced 
contextual bandits on a large number of supervised learning datasets and on a synthetic example that simulates model misspecification and prejudice in the initial training data.<br>
[<a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4221/4099" target="_blank">Paper</a>][<a href="assets/docs/AAAI2019_poster.pdf">Poster</a>]
</p>
								</div>
							</div>
							
<div class="feature">
								<div class="image rounded">
								  <img src="images/design.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>On the Design of Estimators for Bandit Off-Policy Evaluation</h4>
									</header>
<p><i>Nikos&nbsp;Vlassis, Aurelien&nbsp;Bibaut, <br> Maria&nbsp;Dimakopoulou, Tony&nbsp;Jebara (ICML&nbsp;2019)</i></p>
Off-policy evaluation is the problem of estimating
the value of a target policy using data collected
under a different policy. We describe a
framework for designing estimators for bandit off-policy
evaluation. Given a base estimator and a
parametrized class of control variates, we seek a
control variate in that class that reduces the risk
of the base estimator. We derive the population
risk as a function of the class parameters and we
discuss approaches for optimizing this function.
We present our main results in the context
of multi-armed bandits, and we decribe a contextual bandits estimator that is shown to perform well in
multi-class cost-sensitive classification datasets.<br>
[<a href="assets/docs/ICML2019.pdf">Paper</a>]
</p>
								</div>
							</div>
				

							
													<div class="feature">
								<div class="image rounded">
								  <img src="images/robots.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Scalable Coordinated Exploration in Concurrent Reinforcement Learning</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Ian&nbsp;Osband, Benjamin&nbsp;Van&nbsp;Roy (NeurIPS&nbsp;2018)</i></p>
We consider a team of reinforcement learning agents that concurrently operate in a
common environment, and we develop an approach to efficient coordinated exploration
that is suitable for problems of practical scale. Our approach builds on seed sampling (Dimakopoulou and Van Roy, 2018)
and randomized value function learning (Osband et al., 2016). We demonstrate that, for simple tabular
contexts, the approach is competitive with previously proposed tabular model learning
methods (Dimakopoulou and Van Roy, 2018). With a higher-dimensional problem and a neural network value function
representation, the approach learns quickly with far fewer agents than alternative
exploration schemes.<br>
[<a href="https://arxiv.org/pdf/1805.08948.pdf" target="_blank">Paper</a>][<a href="assets/docs/NIPS2018_poster.pdf">Poster</a>]
</p>
								</div>
							</div>




							<div class="feature">
								<div class="image rounded">
								  <img src="images/cubist.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Coordinated Exploration in Concurrent Reinforcement Learning</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Benjamin&nbsp;Van&nbsp;Roy (ICML&nbsp;2018;&nbsp;long&nbsp;talk)</i></p>
<p>We consider a team of reinforcement learning agents that concurrently learn to operate in a common environment, while sharing data in real-time. 
We identify three properties that are essential to efficient coordinated exploration: real-time adaptivity to shared observations, 
commitment to carry through with action sequences that reveal new information, and diversity across learning opportunities pursued by different agents. 
We demonstrate that optimism-based approaches fall short with respect to diversity, while naive extensions of Thompson sampling lack commitment. 
We propose seed sampling that offers a general approach to designing effective coordination algorithms for concurrent reinforcement learning 
and has substantial advantages over alternative exploration schemes.<br>
[<a href="http://proceedings.mlr.press/v80/dimakopoulou18a/dimakopoulou18a.pdf" target="_blank">Paper</a>]
[<a href="https://youtu.be/xjGK-wm0PkI" target="_blank">Demo</a>]
[<a href="https://docs.google.com/presentation/d/1tKztxH5FaH2IRVtCu1escC3O51u26ok05A3xlSSwWAY/edit#slide=id.g3d32e7e0e3_0_129" target="_blank">ICML 2018 Slides</a>] [<a href="https://www.facebook.com/icml.imls/videos/429761510871703/">ICML 2018 Video</a>]
</p>
								</div>
							</div>
							
							
		
							<div class="feature">
								<div class="image rounded">
								  <img src="images/thinker.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Estimation Considerations in Contextual Bandits</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Zhengyuan&nbsp;Zhou, Susan&nbsp;Athey, Guido&nbsp;Imbens</i></p>
<p> We study a new consideration for the exploration vs. exploitation framework which is that the way exploration is 
conducted in the present may affect the bias and variance in the potential outcome model estimation in subsequent stages of learning. 
We show that contextual bandit algorithms are sensitive to the estimation method of the outcome model as well as the exploration method used,
 particularly in the presence of rich heterogeneity or complex outcome models, which can lead to difficult estimation problems along the path of learning. 
 We propose new contextual bandit designs, combining parametric and nonparametric statistical estimation methods with causal inference methods in order to 
 reduce the estimation bias and provide empirical evidence that guides the choice among the alternatives in different scenarios.<br>
[<a href="https://arxiv.org/abs/1711.07077" target="_blank">Paper</a>]
</p>
								</div>
							</div>
							
														<div class="feature">
								<div class="image rounded">
								  <img src="images/wireless.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Market-based dynamic service mode switching in wireless networks</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, Nicholas&nbsp;Bambos, Martin&nbsp;Valdez-Vivas, John&nbsp;Apostolopoulos (PIMRC&nbsp;2017)</i></p>
<p>We consider a virtualized wireless networking architecture, where infrastructure access points of different carriers form a marketplace of resources and bid service deals to a mobile device. At each point in time the mobile evaluates the available service deals and dynamically decides which one to accept and use in the next transmission interval. Its objective is to minimize the long term cumulative service cost and latency cost to transmit packets in its buffer. We develop a model of this architecture, which allows for the formulation and computation of the optimal control for the mobile to accept an offered deal amongst many and switch into the corresponding service mode. The performance of the optimal
and low-complexity heuristic controls is probed via simulation.<br>
[<a href="http://ieeexplore.ieee.org/document/8292195/" target="_blank">Paper</a>]
</p>
								</div>
							</div>
							<div class="feature">
								<div class="image rounded">
								  <img src="images/dc.jpg" alt="" /></div>
								<div class="content">
									<header>
										<h4>Reliable and Efficient Performance Monitoring in Linux</h4>
									</header>
<p><i>Maria&nbsp;Dimakopoulou, St&eacute;phane&nbsp;Eranian, Nectarios&nbsp;Koziris, Nicholas&nbsp;Bambos (Supercomputing&nbsp;2016)</i></p>
<p>We address a published eratum in the Performance Monitoring Unit (PMU) of Intel Sandy Bridge, Ivy Bridge and Haswell processors with hyper-threading enabled which causes cross hyper-thread hardware counter corruption and may produce unreliable results. We propose a cache-coherence style protocol, which we implement in the Linux kernel to address the issue by introducing cross hyper-thread dynamic event scheduling. Additionally, we improve event scheduling efficiency by introducing a bipartite graph matching algorithm which optimally schedules events onto hardware counters consistently. The improvements have been contributed to the upstream Linux kernel&nbsp;v4.1.<br>
[<a href="http://ieeexplore.ieee.org/document/7877112/" target="_blank">Paper</a>]
</p>
							
							
							
						</div>
					</div>
				</section>
			<!-- Four -->
				<section id="four" class="wrapper style3 special">
					<div class="container">
						<header class="major">
							<h2>Get in touch</h2>
						</header>
					</div>
				</section>

		<!-- Footer -->
			<footer id="footer">
				<div class="container">
					<ul class="icons">
						<li><a href="mailto:madima@stanford.edu" class="icon fa-mail-reply"></a></li>
						<!-- <li><a href="https://github.com/mdimakopoulou" class="icon fa-github"></a></li> -->
						<li><a href="https://www.linkedin.com/in/maria-dimakopoulou-4567428a/" class="icon fa-linkedin"></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Maria Dimakopoulou</li>
						<li>Design: <a href="http://templated.co">TEMPLATED</a></li>
						<!-- <li>Images: <a href="http://unsplash.com">Unsplash</a></li> -->
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
